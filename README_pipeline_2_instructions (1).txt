# Functions Overview â€” LangGraph LLM Pipeline

This module contains the following functions:

- count_tokens
- extract_dataframe_info
- llm_generate_code_node
- execute_code_node
- llm_summarize_node
- build_prompt
- run_query_with_retries


---
Function Summaries

count_tokens(text, model_name="gpt-4")
Counts the number of tokens in a given text string for the specified OpenAI model using the `tiktoken` library.

 extract_dataframe_info(df, year, questions_col='question_label', macro_col='macro_label')
Extracts schema and metadata from a yearly DataFrame, including column names/dtypes, unique question labels, and macro labels.

llm_generate_code_node(state: State) -> State
LangGraph node that builds a schema-aware prompt, calls the selected LLM (e.g., GPT-4, GPT-4-Turbo, GPT-3.5-Turbo), and extracts generated Python code from the output.

 execute_code_node(state: State) -> State
Executes the Python code generated by the LLM in an isolated local variable context, capturing `result` or any execution error.

 llm_summarize_node(state: State) -> State
LangGraph node that uses the LLM to summarize execution results into a human-readable answer based on the original query.

 build_prompt(dfs_info, query_placeholder="{query}", feedback: str = "", error: str = "")
Constructs the schema-aware prompt string from provided DataFrame info, optionally adding user feedback and error context.

run_query_with_retries(query, builder, max_retries=5, interactive_feedback=True)
Executes a LangGraph pipeline with retry logic, feeding back execution errors to the LLM for iterative correction, and supports interactive human feedback.

---

## Usage
Import these functions into your pipeline script to integrate them with the LangGraph state graph as shown in the full pipeline README.

USE YOUR OWN API KEY AS IT HAS BEEN REMOVED FROM PARTICULAR CELLS AND MODE OF USE THEM HAS BEEN SET TO INSERTION INTO PARTICULAR CALLABLE CELLS INSTED OF 
ENVIRONMENT FOR NOW.